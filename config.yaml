# llama_model:
#   model_path: 'models\llama-2-7b-chat.Q4_K_M.gguf'
#   model_type: 'llama'
#   model_config: {'max_new_tokens': 512, 'temperature': 0.2, 'context_length': 4096, 'gpu_layers': 5}
mistral_model:
  model_path: 'D:\models\mistral-7b-instruct-v0.1.Q5_K_M.gguf'
  model_type: 'mistral'
  model_config: {'max_new_tokens': 512, 'temperature': 0.2, 'context_length': 4096, 'gpu_layers': 10 }
  model_name: 'mistral-7b-instruct-v0.1'
tinyllama_model:
  model_path: 'D:\models\tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf'
  model_type: 'tinyllama'
  model_config: {'max_new_tokens': 512, 'temperature': 0.2, 'context_length': 2048, 'gpu_layers': 10 }
  model_name: 'tinyllama-1.1b-chat-v1.0'



embeddings_path: 'BAAI/bge-large-en-v1.5'




chat_history_path: 'E:\doc-support\chat_sessions\'

vector_db_path: 'E:\doc-support\chroma_db'